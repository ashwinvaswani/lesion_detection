{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DenseNet121\n",
    "Difference from densenet in torchvision for higher resolution:\n",
    "1. Modify the stride of first convolution layer (7x7 with stride 2) into 1  \n",
    "2. Remove the first max-pooling layer\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.densenet import model_urls\n",
    "import math\n",
    "from mmdet.models.registry import BACKBONES\n",
    "from alignshift.operators.acsconv import ACSConv\n",
    "import torch.utils.checkpoint as cp\n",
    "from mmdet.models.utils import build_conv_layer, build_norm_layer\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
    "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
    "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
    "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
    "}\n",
    "\n",
    "norm_cfg = dict(type='SyncBN')\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "def densenet121(pretrained=False, **kwargs):\n",
    "    r\"\"\"Densenet-121 model from\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                     **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
    "        model_state_dict = model.state_dict()\n",
    "        online_sd = list(state_dict.items())\n",
    "        count = 0\n",
    "        for i, k in enumerate(model_state_dict.keys()):\n",
    "            if 'num_batches_tracked' not in k:\n",
    "                print(i, count, k, online_sd[count][0])\n",
    "                model_state_dict[k] = online_sd[count][1]\n",
    "                count += 1\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        print('densenet loaded imagenet pretrained weights')\n",
    "    else:\n",
    "        print('densenet without imagenet pretrained weights')\n",
    "    return model\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, n_fold, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', build_norm_layer(norm_cfg, num_input_features, postfix=1)[1]),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', ACSConv(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False, n_fold=n_fold)),\n",
    "        self.add_module('norm2', build_norm_layer(norm_cfg, num_input_features, postfix=1)[1]),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', ACSConv(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False, n_fold=n_fold)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, n_fold, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "                n_fold=n_fold,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', ACSConv(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool3d(kernel_size=[1, 2, 2], stride=[1, 2, 2]))#, padding=[0,1,1]\n",
    "\n",
    "\n",
    "class _Reduction_z(nn.Sequential):\n",
    "    def __init__(self, input_features, input_slice):\n",
    "        super().__init__()\n",
    "        self.add_module('reduction_z_conv', nn.Conv3d(input_features, input_features, kernel_size=[input_slice, 1, 1],\n",
    "                                                    stride=1, bias=False))\n",
    "        \n",
    "\n",
    "@BACKBONES.register_module\n",
    "class DenseNetCustomTrunc3dACS(nn.Module):\n",
    "    def __init__(self, \n",
    "                out_dim=256,\n",
    "                n_cts=3,\n",
    "                fpn_finest_layer=1,\n",
    "                memory_efficient=True,\n",
    "                n_fold=8,):\n",
    "        super().__init__()\n",
    "        self.depth = 121\n",
    "        self.feature_upsample = True\n",
    "        self.fpn_finest_layer = fpn_finest_layer\n",
    "        self.out_dim = out_dim\n",
    "        self.n_cts = n_cts\n",
    "        self.mid_ct = n_cts//2\n",
    "        self.n_fold = n_fold\n",
    "        assert self.depth in [121]\n",
    "        if self.depth == 121:\n",
    "            num_init_features = 64\n",
    "            growth_rate = 32\n",
    "            block_config = (6, 12, 24)\n",
    "            self.in_dim = [64, 256, 512, 1024]\n",
    "        bn_size = 4\n",
    "        drop_rate = 0\n",
    "\n",
    "        # First convolution\n",
    "        self.conv0 = ACSConv(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.norm0 = build_norm_layer(norm_cfg, num_init_features, postfix=1)[1]\n",
    "        self.relu0 = nn.ReLU(inplace=True)\n",
    "        self.pool0 = nn.MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1])\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate,\n",
    "                                n_fold=self.n_fold, memory_efficient=memory_efficient)\n",
    "            self.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            reductionz = _Reduction_z(num_features, self.n_cts)\n",
    "            self.add_module('reductionz%d' % (i + 1), reductionz)\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        # self.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        if self.feature_upsample:\n",
    "            for p in range(4, self.fpn_finest_layer - 1, -1):\n",
    "                layer = nn.Conv2d(self.in_dim[p - 1], self.out_dim, 1)\n",
    "                name = 'lateral%d' % p\n",
    "                self.add_module(name, layer)\n",
    "\n",
    "                nn.init.kaiming_uniform_(layer.weight, a=1)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "        self.init_weights()\n",
    "        # if syncbn:\n",
    "        #     self = nn.SyncBatchNorm.convert_sync_batchnorm(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.norm0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.pool0(x)\n",
    "\n",
    "        x = self.denseblock1(x)\n",
    "        redc1 = self.reductionz1(x)\n",
    "        x = self.transition1(x)\n",
    "\n",
    "\n",
    "        x = self.denseblock2(x)\n",
    "        redc2 = self.reductionz2(x)\n",
    "        x = self.transition2(x)\n",
    "\n",
    "\n",
    "        x = self.denseblock3(x)\n",
    "        redc3 = self.reductionz3(x)\n",
    "        # truncated since here since we find it works better in DeepLesion\n",
    "        # ts3 = self.transition3(db3)\n",
    "        # db4 = self.denseblock4(ts3)\n",
    "\n",
    "        # if self.feature_upsample:\n",
    "        ftmaps = [None, redc1.squeeze(2), redc2.squeeze(2), redc3.squeeze(2)]\n",
    "        x = self.lateral4(ftmaps[-1])\n",
    "        for p in range(3, self.fpn_finest_layer - 1, -1):\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            y = ftmaps[p-1]\n",
    "            lateral = getattr(self, 'lateral%d' % p)(y)\n",
    "            x += lateral\n",
    "        return [x]\n",
    "\n",
    "    def init_weights(self, pretrained=True):\n",
    "        pattern = re.compile(\n",
    "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "        state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
    "        for key in list(state_dict.keys()):\n",
    "            res = pattern.match(key)\n",
    "            if res:\n",
    "                new_key = res.group(1) + res.group(2)\n",
    "                state_dict[new_key] = state_dict[key]\n",
    "                del state_dict[key]\n",
    "        state_dict1 = {}\n",
    "        for key in list(state_dict.keys()):\n",
    "            new_key = key.replace('features.', '')\n",
    "            if state_dict[key].dim() == 4:           \n",
    "                t0 = state_dict[key].shape[1]\n",
    "                state_dict1[new_key] = state_dict[key]#.unsqueeze(2)#.repeat((1,1,self.n_cts,1,1))/self.n_cts\n",
    "                if t0 == 3:\n",
    "                    state_dict1[new_key] = state_dict1[new_key][:,1:2,...]\n",
    "            else:\n",
    "                state_dict1[new_key] = state_dict[key]\n",
    "\n",
    "        self.load_state_dict(state_dict1, strict=False)\n",
    "\n",
    "    def freeze(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            print('freezing', name)\n",
    "            param.requires_grad = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
